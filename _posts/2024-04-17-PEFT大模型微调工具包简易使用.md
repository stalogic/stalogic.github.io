---
layout:     post
title:      PEFT大模型微调工具包简易使用
subtitle:    "\"Parameter-Efficient Fine-Tuning\""
date:       2024-04-17
author:     Eason
header-img: img/post-bg-tulip.jpg
catalog: true
tags:
    - PEFT
    - Finetune
    - LLM
---



## 前言

大模型（Large Language Models, LLM）的模型参数基本都是以**B**illion为单位，使得重新训练模型、或者对全量参数的微调都会很耗时，导致个人、使用民用级别的显卡微调大模型难以进行。一个办法就是使用**Parameter-Efficient Fine-Tuning**（PEFT）来微调模型，这些方法只对模型的一部分参数进行微调，可以节省训练时间，并且可以节省显存。

---

### 安装&环境配置

为了避免污染系统Python环境，我们使用`conda`创建虚拟环境来安装依赖。
```bash
conda create -n peft python=3.10  # 创建虚拟环境
conda activate peft # 激活虚拟环境
pip install peft transformers torch accelerate datasets pandas  # 安装依赖
```


### 创建PEFT模型

创建一个PEFT模型非常简单，只需要提供一个`transformers`模型和一个`peft`配置（LoraConfig，PrefixTuningConfig），然后使用`get_peft_model`函数就可以得到相应的模型。
```python
from peft import LoraConfig, TaskType

peft_config = LoraConfig(   # 创建LoraConfig
    task_type=TaskType.CAUSAL_LM,   # 指定任务类型
    inference_mode=False,   # 是否为推理模式
    r=8,    # 秩（rank）
    lora_alpha=32,  # lora_alpha参数
    lora_dropout=0.1,   # lora_dropout参数
)

from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(   # 创建模型
    "THUDM/chatglm3-6b",    # 模型路径
    trust_remote_code=True, # 允许远程代码
    load_in_4bit=True,  # 4bit量化
    device_map="auto",  # 设备映射
)

from peft import get_peft_model
model = get_peft_model(model, peft_config)  # 创建PEFT模型
model.print_trainable_parameters()  # 打印可训练参数
```

### 构建训练数据
大模型Finetune的一般是**指令微调（Instruction-Finetune）**，训练数据一般有三个字段，分别是`instruction`（指令）、`input`（输入）和`output`（输出），其中`instruction`字段一般是任务介绍，比如文本总结、文本续写等指令信息，`input`字段是具体的任务文本信息，`output`字段是模型根据指令和输入生成输出，`instruction`和`input`字段有时候是可以合并在一起的。

```json
{"input": "妮露#关于提纳里", "output": "据说他是很厉害的植物学家。那他分辨蘑菇的本领肯定很厉害，至少不会吃到毒蘑菇…好羡慕呀。"}
{"input": "妮露#关于柯莱", "output": "她偶尔会来看我的演出，是一位很热心的见习巡林员呢。她还提醒过我小心迷路，这是为什么呢？我也没有迷路过呀。"}
```

#### 训练模板与Tokenizer

每个大模型都有自己的输入模板，一般都会有多种角色，比如`System`，`User`，`Assistant`等，这些在不同大模型中都有不同的关键词，此外每个大模型都有自己定义的特殊token。另外，大模型输入的长文本经过分词，通过**tokenizing**过程会得到整数序列，这个过程类似于查表，每个大模型都有自己的词表，**tokenizing**过程把每个词对应到一个整数，这个整数就是`token`。对不同大模型进行Finetune，都需要根据模板和Tokenizer进行适配。

##### 读取JSON数据并构建Dataset
```python
def get_dataframe():
    import pandas as pd
    import json
    df = pd.DataFrame()
    with open("path/to/data.jsonl") as fr:
        for line in fr:
            data = json.loads(line)
            df = df._append(data, ignore_index=True)
    return df

df = get_dataframe()
from datasets import Dataset
ds = Dataset.from_pandas(df)
```

##### 获取模板和Tokenizer

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(  # 创建Tokenizer
    "THUDM/chatglm3-6b",    # 模型路径
    trust_remote_code=True, # 允许远程代码
)

# ChatGLM3的模板
def process_func(example):  # 处理函数， example是数据集中的一行数据
    MAX_LENGTH = 512
    input_ids, labels = [], []
    prompt = [tokenizer.get_command("<|system|>")] + tokenizer.encode("现在模仿原神角色评价他人\n ", add_special_tokens=False)
    if "instruction" in example:
        instruction_ = [tokenizer.get_command("<|user|>")] + tokenizer.encode("\n " + "\n".join([example["instruction"], example["input"]]).strip(), add_special_tokens=False, max_length=MAX_LENGTH) + [tokenizer.get_command("<|assistant|>")]
    else:
        instruction_ = [tokenizer.get_command("<|user|>")] + tokenizer.encode("\n " + example["input"].strip(), add_special_tokens=False, max_length=MAX_LENGTH) + [tokenizer.get_command("<|assistant|>")]
    instruction = tokenizer.encode(prompt + instruction_)
    response = tokenizer.encode("\n" + example["output"], add_special_tokens=False)
    input_ids = instruction + response + [tokenizer.eos_token_id]
    labels = [tokenizer.pad_token_id] * len(instruction) + response + [tokenizer.eos_token_id]
    pad_len = MAX_LENGTH - len(input_ids)
    input_ids += [tokenizer.pad_token_id] * pad_len
    labels += [tokenizer.pad_token_id] * pad_len
    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]

    return {
        "input_ids": input_ids,
        "labels": labels
    }
```

##### Tokenized Dataset

```python
tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)
```

### 模型训练

在构建了模型和数据后，就可以开始训练了。在这个过程中，需要设置超参数，`transformers`中提供了`TrainingArguments`类，可以设置训练的超参数，包括`per_device_train_batch_size`、`per_device_eval_batch_size`、`gradient_accumulation_steps`、`learning_rate`、`num_train_epochs`等。

```python
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir="path/to/save/model-and-logs",          # output directory
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    logging_steps=20,
    num_train_epochs=1
)
```

然后创建一个`Trainer`对象，并调用`train`方法进行训练并保存模型。这里的`DataCollatorForSeq2Seq`是`transformers`中提供的一个数据预处理类，用于动态填充数据，避免数据填充导致的显存占用过多问题。
```python
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(
    tokenizer, 
    model=model,
    label_pad_token_id=-100,
    pad_to_multiple_of=None,
    padding=False
    )

from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds,
    data_collator=data_collator,
)

trainer.train()
model.save_pretrained(training_args.output_dir)
```


### 运行模型

通过命令运行python脚本，可以看到下面的运行结果，这里使用使用2070显卡进行训练，从日志可以看到，迭代一次需要将近10分钟。

```bash
/home/eason/.miniconda3/envs/peft/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead:

dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
  0%|                                                                                                | 0/89 [00:00<?, ?it/s]/home/eason/.miniconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(
 13%|██████████▉                                                                      | 12/89 [1:57:16<13:16:15, 620.46s/it]
```